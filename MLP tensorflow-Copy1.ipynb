{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 2})\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMoreData(dataframe, X, y, label, multiply):\n",
    "    # EG. 55x6=330\n",
    "    pos_list = [i for i, x in enumerate(dataframe.label) if x == label]\n",
    "    \n",
    "    more_data_x_pos = []\n",
    "\n",
    "    for x in range(multiply):\n",
    "        for pos in pos_list:\n",
    "            more_data_x_pos.append(X[pos])\n",
    "    \n",
    "    more_data_y_pos = [1 for i in range(len(more_data_x_pos))]\n",
    "    \n",
    "    y = np.concatenate((y, more_data_y_pos), axis=None)\n",
    "\n",
    "    for x in more_data_x_pos:\n",
    "        X.append(x)\n",
    "        \n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus_path = \"/datb/aphasia/languagedata/corpus/dataset/datasetboundary_distance_4ms_v1.csv\"\n",
    "\n",
    "df_corpus_data = pd.read_csv(data_corpus_path, sep=',', skiprows=1,\n",
    "                             names=['region', 'label', 'sample_rate', 'begin', 'end', 'audiopath'])\n",
    "\n",
    "# Voor het weghalen van 'nan' rows in de column \"region\"\n",
    "df_corpus = df_corpus_data.dropna(subset=['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1996214, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before generating more 1 labels data\n",
      "X:1996214\n",
      "y:1996214\n",
      "After generating more 1 labels data\n",
      "X:3629480\n",
      "y:3629480\n"
     ]
    }
   ],
   "source": [
    "# # Get input data\n",
    "X = []\n",
    "\n",
    "for x in df_corpus.region:\n",
    "    trans = [float(y) for y in x.split('|')] # 0,1\n",
    "    X.append(trans)\n",
    "\n",
    "# # Get labels\n",
    "y = [int(x) for x in df_corpus.label]\n",
    "\n",
    "print('Before generating more 1 labels data')\n",
    "print('X:{}'.format(len(X)))\n",
    "print('y:{}'.format(len(y)))\n",
    "\n",
    "more_data = generateMoreData(df_corpus, X, y, 1, 9)\n",
    "\n",
    "X_more = more_data[0]\n",
    "y_more = more_data[1]\n",
    "\n",
    "print('After generating more 1 labels data')\n",
    "print('X:{}'.format(len(X_more)))\n",
    "print('y:{}'.format(len(y_more)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X more cut size: 300000\n",
      "Y more cut size: 300000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 150000, 0: 150000})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snij de dataset\n",
    "import collections\n",
    "\n",
    "def generateBalancedData(X_more, y_more, size):\n",
    "    pos_list = [i for i, x in enumerate(y_more) if x == 1][:int(size/2)]\n",
    "    neg_list = [i for i, x in enumerate(y_more) if x == 0][:int(size/2)]\n",
    "\n",
    "    balancedDataX = np.concatenate(([X_more[pos] for pos in pos_list], [X_more[neg] for neg in neg_list]), axis=0)\n",
    "    \n",
    "    balancedDataY = np.concatenate(([y_more[pos] for pos in pos_list], [y_more[neg] for neg in neg_list]), axis=0)\n",
    "        \n",
    "    return [balancedDataX, balancedDataY]\n",
    "\n",
    "\n",
    "\n",
    "dataSize = 300000\n",
    "\n",
    "balancedX, balancedY = generateBalancedData(X_more, y_more, dataSize)\n",
    "\n",
    "X_more_cut = balancedX\n",
    "y_more_cut = balancedY\n",
    "\n",
    "print('X more cut size: {}'.format(len(X_more_cut)))\n",
    "print('Y more cut size: {}'.format(len(y_more_cut)))\n",
    "\n",
    "collections.Counter(y_more_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_more) == len(y_more)\n",
    "assert len(X_more_cut) == len(y_more_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "LEARNING_RATE = 0.005 # probeer 0.030, 0.010, 0.003, 0.001\n",
    "N_INSTANCES = len(X_more_cut)\n",
    "TEST_SIZE = 0.2\n",
    "TRAIN_SIZE = int(N_INSTANCES * (1 - TEST_SIZE)) \n",
    "BATCH_SIZE = 100\n",
    "ACTIVATION_FUNCTION_SIGMOID = tf.nn.sigmoid\n",
    "STDDEV = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Network Parameters\n",
    "# hidden_nodes = 100\n",
    "hidden_nodes = 10\n",
    "num_classes = 2\n",
    "num_features = len(X_more_cut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for labels\n",
    "labels_ = np.zeros((N_INSTANCES, num_classes))\n",
    "labels_[np.arange(N_INSTANCES), y_more_cut] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_more_cut, labels_,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(240000, 52)\n",
      "X_test:(60000, 52)\n",
      "y_train:(240000, 2)\n",
      "y_test:(60000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:{}'.format(X_train.shape))\n",
    "print('X_test:{}'.format(X_test.shape))\n",
    "print('y_train:{}'.format(y_train.shape))\n",
    "print('y_test:{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = hidden_nodes # 1st layer number of neurons\n",
    "n_hidden_2 = hidden_nodes # 2nd layer number of neurons\n",
    "n_hidden_3 = hidden_nodes # 2nd layer number of neurons\n",
    "n_hidden_4 = hidden_nodes # 2nd layer number of neurons\n",
    "n_hidden_5 = hidden_nodes # 2nd layer number of neurons\n",
    "# n_hidden_3 = hidden_nodes\n",
    "n_input = num_features # CORPUS data input (audio region shape: 65)\n",
    "n_classes = num_classes # CORPUS total classes (0-1 labels)\n",
    "    \n",
    "# placeholders for training pairs (x, y)\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_input], name=\"X\")\n",
    "Y = tf.placeholder(tf.int32, shape=[None, n_classes], name=\"Y\")\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    " \n",
    "    layer1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1']))\n",
    "    layer1 = tf.nn.dropout(layer1, 0.5)\n",
    "    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2']))\n",
    "    layer3 = tf.nn.relu(tf.add(tf.matmul(layer2, _weights['h3']), _biases['b3']))\n",
    "    layer4 = tf.nn.relu(tf.add(tf.matmul(layer3, _weights['h4']), _biases['b4']))\n",
    "    layer5 = tf.nn.relu(tf.add(tf.matmul(layer4, _weights['h5']), _biases['b5']))\n",
    "    out_layer = ACTIVATION_FUNCTION_SIGMOID(tf.matmul(layer5, _weights['out']) + _biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=STDDEV)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4],stddev=STDDEV)),\n",
    "    'h5': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4],stddev=STDDEV)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_5, n_classes],stddev=STDDEV)),                                   \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# weights = {\n",
    "#     'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
    "#     'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
    "#     'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=STDDEV)),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes],stddev=STDDEV)),                                   \n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "# }\n",
    "\n",
    "pred = mlp(X, weights, biases)\n",
    "\n",
    "# use a negative log loss function for logistic regression\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "# configure the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.005\n",
      "hidden_nodes: 10\n",
      "Training epochs: 500\n",
      "TEST_SIZE: 0.2\n",
      "Dataset rows: 300000\n",
      "Dataset features: 52 \n",
      "\n",
      "Epoch: 000/500 cost: 0.692573939\n",
      "Training accuracy: 0.530\n",
      "Epoch: 001/500 cost: 0.690096294\n",
      "Training accuracy: 0.570\n",
      "Epoch: 002/500 cost: 0.687699007\n",
      "Training accuracy: 0.420\n",
      "Epoch: 003/500 cost: 0.685383779\n",
      "Training accuracy: 0.590\n",
      "Epoch: 004/500 cost: 0.683429468\n",
      "Training accuracy: 0.530\n",
      "Epoch: 005/500 cost: 0.681996382\n",
      "Training accuracy: 0.560\n",
      "Epoch: 006/500 cost: 0.680355246\n",
      "Training accuracy: 0.470\n",
      "Epoch: 007/500 cost: 0.678418424\n",
      "Training accuracy: 0.560\n",
      "Epoch: 008/500 cost: 0.676528774\n",
      "Training accuracy: 0.450\n",
      "Epoch: 009/500 cost: 0.675542542\n",
      "Training accuracy: 0.640\n",
      "Epoch: 010/500 cost: 0.674670691\n",
      "Training accuracy: 0.590\n",
      "Epoch: 011/500 cost: 0.673418228\n",
      "Training accuracy: 0.560\n",
      "Epoch: 012/500 cost: 0.673022943\n",
      "Training accuracy: 0.590\n",
      "Epoch: 013/500 cost: 0.671277694\n",
      "Training accuracy: 0.530\n",
      "Epoch: 014/500 cost: 0.670223132\n",
      "Training accuracy: 0.540\n",
      "Epoch: 015/500 cost: 0.670091577\n",
      "Training accuracy: 0.540\n",
      "Epoch: 016/500 cost: 0.670169135\n",
      "Training accuracy: 0.560\n",
      "Epoch: 017/500 cost: 0.668635846\n",
      "Training accuracy: 0.610\n",
      "Epoch: 018/500 cost: 0.668359667\n",
      "Training accuracy: 0.590\n",
      "Epoch: 019/500 cost: 0.668245905\n",
      "Training accuracy: 0.520\n",
      "Epoch: 020/500 cost: 0.667994211\n",
      "Training accuracy: 0.570\n",
      "Epoch: 021/500 cost: 0.667227221\n",
      "Training accuracy: 0.600\n",
      "Epoch: 022/500 cost: 0.666611290\n",
      "Training accuracy: 0.640\n",
      "Epoch: 023/500 cost: 0.666104336\n",
      "Training accuracy: 0.560\n",
      "Epoch: 024/500 cost: 0.666015252\n",
      "Training accuracy: 0.610\n",
      "Epoch: 025/500 cost: 0.665843984\n",
      "Training accuracy: 0.630\n",
      "Epoch: 026/500 cost: 0.665690288\n",
      "Training accuracy: 0.580\n",
      "Epoch: 027/500 cost: 0.665163650\n",
      "Training accuracy: 0.650\n",
      "Epoch: 028/500 cost: 0.665172039\n",
      "Training accuracy: 0.550\n",
      "Epoch: 029/500 cost: 0.665167565\n",
      "Training accuracy: 0.660\n",
      "Epoch: 030/500 cost: 0.663778217\n",
      "Training accuracy: 0.580\n",
      "Epoch: 031/500 cost: 0.663720591\n",
      "Training accuracy: 0.580\n",
      "Epoch: 032/500 cost: 0.664264312\n",
      "Training accuracy: 0.560\n",
      "Epoch: 033/500 cost: 0.663080718\n",
      "Training accuracy: 0.550\n",
      "Epoch: 034/500 cost: 0.662954274\n",
      "Training accuracy: 0.560\n",
      "Epoch: 035/500 cost: 0.663869289\n",
      "Training accuracy: 0.580\n",
      "Epoch: 036/500 cost: 0.662591634\n",
      "Training accuracy: 0.560\n",
      "Epoch: 037/500 cost: 0.663055969\n",
      "Training accuracy: 0.590\n",
      "Epoch: 038/500 cost: 0.662439551\n",
      "Training accuracy: 0.580\n",
      "Epoch: 039/500 cost: 0.661701171\n",
      "Training accuracy: 0.630\n",
      "Epoch: 040/500 cost: 0.662639455\n",
      "Training accuracy: 0.640\n",
      "Epoch: 041/500 cost: 0.661487894\n",
      "Training accuracy: 0.540\n",
      "Epoch: 042/500 cost: 0.661468269\n",
      "Training accuracy: 0.620\n",
      "Epoch: 043/500 cost: 0.661735224\n",
      "Training accuracy: 0.610\n",
      "Epoch: 044/500 cost: 0.661194519\n",
      "Training accuracy: 0.510\n",
      "Epoch: 045/500 cost: 0.660368220\n",
      "Training accuracy: 0.670\n",
      "Epoch: 046/500 cost: 0.660674377\n",
      "Training accuracy: 0.570\n",
      "Epoch: 047/500 cost: 0.660858859\n",
      "Training accuracy: 0.650\n",
      "Epoch: 048/500 cost: 0.660540928\n",
      "Training accuracy: 0.520\n",
      "Epoch: 049/500 cost: 0.660082553\n",
      "Training accuracy: 0.530\n",
      "Epoch: 050/500 cost: 0.659969586\n",
      "Training accuracy: 0.610\n",
      "Epoch: 051/500 cost: 0.660304363\n",
      "Training accuracy: 0.600\n",
      "Epoch: 052/500 cost: 0.659838998\n",
      "Training accuracy: 0.510\n",
      "Epoch: 053/500 cost: 0.659916043\n",
      "Training accuracy: 0.570\n",
      "Epoch: 054/500 cost: 0.660199502\n",
      "Training accuracy: 0.550\n",
      "Epoch: 055/500 cost: 0.659061446\n",
      "Training accuracy: 0.540\n",
      "Epoch: 056/500 cost: 0.659386341\n",
      "Training accuracy: 0.610\n",
      "Epoch: 057/500 cost: 0.659561076\n",
      "Training accuracy: 0.580\n",
      "Epoch: 058/500 cost: 0.659968829\n",
      "Training accuracy: 0.690\n",
      "Epoch: 059/500 cost: 0.659469190\n",
      "Training accuracy: 0.680\n",
      "Epoch: 060/500 cost: 0.659714728\n",
      "Training accuracy: 0.570\n",
      "Epoch: 061/500 cost: 0.659278504\n",
      "Training accuracy: 0.580\n",
      "Epoch: 062/500 cost: 0.659439093\n",
      "Training accuracy: 0.620\n",
      "Epoch: 063/500 cost: 0.658797976\n",
      "Training accuracy: 0.600\n",
      "Epoch: 064/500 cost: 0.658663288\n",
      "Training accuracy: 0.490\n",
      "Epoch: 065/500 cost: 0.658552370\n",
      "Training accuracy: 0.540\n",
      "Epoch: 066/500 cost: 0.658014183\n",
      "Training accuracy: 0.630\n",
      "Epoch: 067/500 cost: 0.659110412\n",
      "Training accuracy: 0.590\n",
      "Epoch: 068/500 cost: 0.658457642\n",
      "Training accuracy: 0.570\n",
      "Epoch: 069/500 cost: 0.658351679\n",
      "Training accuracy: 0.590\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 500\n",
    "# training_epochs = 10000\n",
    "display_step = 1 # controls how often the loss is reported\n",
    "\n",
    "modeltrain_log = '/datb/aphasia/languagedata/corpus/result/modeltrain_log_h_2_n_82'\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "#     with open(modeltrain_log+'.csv', 'w') as writeTo:\n",
    "        \n",
    "#         writer = csv.DictWriter(writeTo, fieldnames=['epoch', 'training_acc', 'test_acc', 'avg_cost'])\n",
    "#         writer.writeheader()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print('Learning rate: {}'.format(LEARNING_RATE))\n",
    "        print('hidden_nodes: ' + str(hidden_nodes))\n",
    "        print('Training epochs: {}'.format(training_epochs))\n",
    "        print('TEST_SIZE: ' + str(TEST_SIZE))\n",
    "        print('Dataset rows: {}'.format(len(X_more_cut)))\n",
    "        print('Dataset features: {} \\n'.format(len(X_more_cut[0])))\n",
    "\n",
    "        for epoch in range(training_epochs+1):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / BATCH_SIZE)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                randidx = np.random.randint(int(TRAIN_SIZE), size = BATCH_SIZE)\n",
    "                batch_xs = X_train[randidx, :]\n",
    "                batch_ys = y_train[randidx, :]\n",
    "\n",
    "                # Fit using batched data\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "                # Calculate average cost\n",
    "                avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "\n",
    "            # Display progress\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "                train_acc = sess.run(accuracy, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "                \n",
    "                test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "\n",
    "#                 writer.writerow({'epoch': epoch, 'training_acc': train_acc, 'test_acc':test_acc, 'avg_cost': avg_cost})\n",
    "\n",
    "    #             print(sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys}))\n",
    "\n",
    "                # Save the variables to disk.\n",
    "    #             save_path = saver.save(sess, export_path+\"phonemeboundary_model-\"+str(epoch))\n",
    "    #             print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "        print (\"End of training.\\n\")\n",
    "        print(\"Testing...\\n\")\n",
    "        \n",
    "        \n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={X: X_test, Y: y_test})\n",
    "\n",
    "        print(\"validation accuracy: {}\".format(val_accuracy))\n",
    "        y_true = np.argmax(y_test,1)\n",
    "        print(\"Precision: {}\".format(precision_score(y_true, y_pred)))\n",
    "        print(\"Recall:{}\".format(recall_score(y_true, y_pred)))\n",
    "        print(\"f1_score:{}\".format(f1_score(y_true, y_pred)))\n",
    "        \n",
    "        print(\"confusion_matrix\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        \n",
    "        print(\"classification_report\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "    #     # Save the variables to disk.\n",
    "    #     save_path = saver.save(sess, export_path+\"phonemeboundary_model\")\n",
    "    #     print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "        # Testing\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "        print (\"Test accuracy: %.3f\" % (test_acc))\n",
    "\n",
    "        sess.close()\n",
    "        print(\"Session closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "          new_saver = tf.train.import_meta_graph('my_jesse_test2_model-1000.meta')\n",
    "          new_saver.restore(sess, tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltrain_log_path = '/datb/aphasia/languagedata/corpus/result/modeltrain_log.csv'\n",
    "\n",
    "modeltrain_log = pd.read_csv(modeltrain_log_path, sep=',', skiprows=1,\n",
    "                             names=['epoch', 'training_acc', 'test_acc', 'avg_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = modeltrain_log.loc[modeltrain_log['epoch'].idxmax()]\n",
    "train_acc = modeltrain_log.training_acc\n",
    "test_acc = modeltrain_log.test_acc\n",
    "epoch = int(max_values[0])\n",
    "max_train = modeltrain_log.loc[modeltrain_log['training_acc'].idxmax()]\n",
    "max_test = modeltrain_log.loc[modeltrain_log['test_acc'].idxmax()]\n",
    "print('Max train: {}'.format(max_train[1]))\n",
    "print('Max test: {}'.format(max_test[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMoreData(dataframe, X, y, label, multiply):\n",
    "    # EG. 55x6=330\n",
    "    pos_list = [i for i, x in enumerate(dataframe.label) if x == label]\n",
    "    \n",
    "    more_data_x_pos = []\n",
    "\n",
    "    for x in range(multiply):\n",
    "        for pos in pos_list:\n",
    "            more_data_x_pos.append(X[pos])\n",
    "    \n",
    "    more_data_y_pos = [1 for i in range(len(more_data_x_pos))]\n",
    "    \n",
    "    y = np.concatenate((y, more_data_y_pos), axis=None)\n",
    "\n",
    "    for x in more_data_x_pos:\n",
    "        X.append(x)\n",
    "        \n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus_path = \"/datb/aphasia/languagedata/corpus/dataset/datasetboundary_distance_4ms_v1.csv\"\n",
    "\n",
    "df_corpus_data = pd.read_csv(data_corpus_path, sep=',', skiprows=1,\n",
    "                             names=['region', 'label', 'sample_rate', 'begin', 'end', 'audiopath'])\n",
    "\n",
    "# Voor het weghalen van 'nan' rows in de column \"region\"\n",
    "df_corpus = df_corpus_data.dropna(subset=['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1996214, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before generating more 1 labels data\n",
      "X:1996214\n",
      "y:1996214\n",
      "After generating more 1 labels data\n",
      "X:3629480\n",
      "y:3629480\n"
     ]
    }
   ],
   "source": [
    "# # Get input data\n",
    "X = []\n",
    "\n",
    "for x in df_corpus.region:\n",
    "    trans = [float(y) for y in x.split('|')] # 0,1\n",
    "    X.append(trans)\n",
    "\n",
    "# # Get labels\n",
    "y = [int(x) for x in df_corpus.label]\n",
    "\n",
    "print('Before generating more 1 labels data')\n",
    "print('X:{}'.format(len(X)))\n",
    "print('y:{}'.format(len(y)))\n",
    "\n",
    "more_data = generateMoreData(df_corpus, X, y, 1, 9)\n",
    "\n",
    "X_more = more_data[0]\n",
    "y_more = more_data[1]\n",
    "\n",
    "print('After generating more 1 labels data')\n",
    "print('X:{}'.format(len(X_more)))\n",
    "print('y:{}'.format(len(y_more)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X more cut size: 300000\n",
      "Y more cut size: 300000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 150000, 0: 150000})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snij de dataset\n",
    "import collections\n",
    "\n",
    "def generateBalancedData(X_more, y_more, size):\n",
    "    pos_list = [i for i, x in enumerate(y_more) if x == 1][:int(size/2)]\n",
    "    neg_list = [i for i, x in enumerate(y_more) if x == 0][:int(size/2)]\n",
    "\n",
    "    balancedDataX = np.concatenate(([X_more[pos] for pos in pos_list], [X_more[neg] for neg in neg_list]), axis=0)\n",
    "    \n",
    "    balancedDataY = np.concatenate(([y_more[pos] for pos in pos_list], [y_more[neg] for neg in neg_list]), axis=0)\n",
    "        \n",
    "    return [balancedDataX, balancedDataY]\n",
    "\n",
    "\n",
    "\n",
    "dataSize = 300000\n",
    "\n",
    "balancedX, balancedY = generateBalancedData(X_more, y_more, dataSize)\n",
    "\n",
    "X_more_cut = balancedX\n",
    "y_more_cut = balancedY\n",
    "\n",
    "print('X more cut size: {}'.format(len(X_more_cut)))\n",
    "print('Y more cut size: {}'.format(len(y_more_cut)))\n",
    "\n",
    "collections.Counter(y_more_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_more) == len(y_more)\n",
    "assert len(X_more_cut) == len(y_more_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "LEARNING_RATE = 0.03 # probeer 0.030, 0.010, 0.003, 0.001\n",
    "N_INSTANCES = len(X_more_cut)\n",
    "TEST_SIZE = 0.3\n",
    "TRAIN_SIZE = int(N_INSTANCES * (1 - TEST_SIZE)) \n",
    "BATCH_SIZE = 100\n",
    "ACTIVATION_FUNCTION_SIGMOID = tf.nn.sigmoid\n",
    "STDDEV = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Network Parameters\n",
    "# hidden_nodes = 100\n",
    "hidden_nodes = 80\n",
    "num_classes = 2\n",
    "num_features = len(X_more_cut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for labels\n",
    "labels_ = np.zeros((N_INSTANCES, num_classes))\n",
    "labels_[np.arange(N_INSTANCES), y_more_cut] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_more_cut, labels_,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(210000, 52)\n",
      "X_test:(90000, 52)\n",
      "y_train:(210000, 2)\n",
      "y_test:(90000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:{}'.format(X_train.shape))\n",
    "print('X_test:{}'.format(X_test.shape))\n",
    "print('y_train:{}'.format(y_train.shape))\n",
    "print('y_test:{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = hidden_nodes # 1st layer number of neurons\n",
    "n_hidden_2 = hidden_nodes # 2nd layer number of neurons\n",
    "# n_hidden_3 = hidden_nodes\n",
    "n_input = num_features # CORPUS data input (audio region shape: 65)\n",
    "n_classes = num_classes # CORPUS total classes (0-1 labels)\n",
    "    \n",
    "# placeholders for training pairs (x, y)\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_input], name=\"X\")\n",
    "Y = tf.placeholder(tf.int32, shape=[None, n_classes], name=\"Y\")\n",
    "\n",
    "def mlp(_X, _weights, _biases):\n",
    "    layer1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1']))\n",
    "    layer1 = tf.nn.dropout(layer1, 0.5)\n",
    "    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2']))\n",
    "    out_layer = ACTIVATION_FUNCTION_SIGMOID(tf.matmul(layer2, _weights['out']) + _biases['out'])\n",
    "\n",
    "#     layer1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1']))\n",
    "#     layer1 = tf.nn.dropout(layer1, 0.5)\n",
    "#     layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2']))\n",
    "#     layer3 = tf.nn.relu(tf.add(tf.matmul(layer2, _weights['h3']), _biases['b3']))\n",
    "#     out_layer = ACTIVATION_FUNCTION_SIGMOID(tf.matmul(layer3, _weights['out']) + _biases['out'])\n",
    "    return out_layer\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=STDDEV)),                                   \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# weights = {\n",
    "#     'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
    "#     'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
    "#     'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=STDDEV)),\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes],stddev=STDDEV)),                                   \n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "# }\n",
    "\n",
    "pred = mlp(X, weights, biases)\n",
    "\n",
    "# use a negative log loss function for logistic regression\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "# configure the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-42-1cd7486c7504>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-1cd7486c7504>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    writer = csv.DictWriter(writeTo, fieldnames=['epoch', 'training_acc', 'test_acc', 'avg_cost'])\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 500\n",
    "# training_epochs = 10000\n",
    "display_step = 1 # controls how often the loss is reported\n",
    "\n",
    "modeltrain_log = '/datb/aphasia/languagedata/corpus/result/modeltrain_log_h_2_n_8'\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    with open(modeltrain_log+'.csv', 'w') as writeTo:\n",
    "        \n",
    "    writer = csv.DictWriter(writeTo, fieldnames=['epoch', 'training_acc', 'test_acc', 'avg_cost'])\n",
    "     writer.writeheader()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print('Learning rate: {}'.format(LEARNING_RATE))\n",
    "        print('hidden_nodes: ' + str(hidden_nodes))\n",
    "        print('Training epochs: {}'.format(training_epochs))\n",
    "        print('TEST_SIZE: ' + str(TEST_SIZE))\n",
    "        print('Dataset rows: {}'.format(len(X_more_cut)))\n",
    "        print('Dataset features: {} \\n'.format(len(X_more_cut[0])))\n",
    "\n",
    "        for epoch in range(training_epochs+1):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / BATCH_SIZE)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                randidx = np.random.randint(int(TRAIN_SIZE), size = BATCH_SIZE)\n",
    "                batch_xs = X_train[randidx, :]\n",
    "                batch_ys = y_train[randidx, :]\n",
    "\n",
    "                # Fit using batched data\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "                # Calculate average cost\n",
    "                avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys})/total_batch\n",
    "\n",
    "            # Display progress\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "                train_acc = sess.run(accuracy, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "                \n",
    "                test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "\n",
    "#                 writer.writerow({'epoch': epoch, 'training_acc': train_acc, 'test_acc':test_acc, 'avg_cost': avg_cost})\n",
    "\n",
    "    #             print(sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys}))\n",
    "\n",
    "                # Save the variables to disk.\n",
    "    #             save_path = saver.save(sess, export_path+\"phonemeboundary_model-\"+str(epoch))\n",
    "    #             print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "        print (\"End of training.\\n\")\n",
    "        print(\"Testing...\\n\")\n",
    "        \n",
    "        \n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={X: X_test, Y: y_test})\n",
    "\n",
    "        print(\"validation accuracy: {}\".format(val_accuracy))\n",
    "        y_true = np.argmax(y_test,1)\n",
    "        print(\"Precision: {}\".format(precision_score(y_true, y_pred)))\n",
    "        print(\"Recall:{}\".format(recall_score(y_true, y_pred)))\n",
    "        print(\"f1_score:{}\".format(f1_score(y_true, y_pred)))\n",
    "        \n",
    "        print(\"confusion_matrix\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        \n",
    "        print(\"classification_report\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "    #     # Save the variables to disk.\n",
    "    #     save_path = saver.save(sess, export_path+\"phonemeboundary_model\")\n",
    "    #     print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "        # Testing\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "        print (\"Test accuracy: %.3f\" % (test_acc))\n",
    "\n",
    "        sess.close()\n",
    "        print(\"Session closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7ccd2eb8daa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeltrain_log\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriteTo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriteTo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_cost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "     with open(modeltrain_log+'.csv', 'w') as writeTo:\n",
    "        \n",
    "            writer = csv.DictWriter(writeTo, fieldnames=['epoch', 'training_acc', 'test_acc', 'avg_cost'])\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File my_model-1000.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c839f783e9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m           \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model-1000.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m                      \"execution is enabled.\")\n\u001b[1;32m   1802\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File my_model-1000.meta does not exist."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "          new_saver = tf.train.import_meta_graph('my_model-1000.meta')\n",
    "          new_saver.restore(sess, tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltrain_log_path = '/datb/aphasia/languagedata/corpus/result/modeltrain_log.csv'\n",
    "\n",
    "modeltrain_log = pd.read_csv(modeltrain_log_path, sep=',', skiprows=1,\n",
    "                             names=['epoch', 'training_acc', 'test_acc', 'avg_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train: 0.7164\n",
      "Max test: 0.5936\n"
     ]
    }
   ],
   "source": [
    "max_values = modeltrain_log.loc[modeltrain_log['epoch'].idxmax()]\n",
    "train_acc = modeltrain_log.training_acc\n",
    "test_acc = modeltrain_log.test_acc\n",
    "epoch = int(max_values[0])\n",
    "max_train = modeltrain_log.loc[modeltrain_log['training_acc'].idxmax()]\n",
    "max_test = modeltrain_log.loc[modeltrain_log['test_acc'].idxmax()]\n",
    "print('Max train: {}'.format(max_train[1]))\n",
    "print('Max test: {}'.format(max_test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
